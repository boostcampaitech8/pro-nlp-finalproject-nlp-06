"""
í†µí•© ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ë° ChromaDB ì €ì¥ íŒŒì´í”„ë¼ì¸
- ì‹œí™©/ì¢…ëª©/ê²½ì œ/ì‚°ì—… ë¦¬í¬íŠ¸ ìë™ ìˆ˜ì§‘
- vLLM ìš”ì•½
- ChromaDB ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import fitz
from openai import OpenAI
import time
import json
from datetime import datetime
import os
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from typing import List, Dict, Optional


class NaverReportPipeline:
    """ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬í¬íŠ¸ í†µí•© íŒŒì´í”„ë¼ì¸"""
    
    # ì¹´í…Œê³ ë¦¬ë³„ URL ë° DB ê²½ë¡œ ë§¤í•‘
    CATEGORIES = {
        "market": {
            "name": "ì‹œí™©",
            "url": "https://finance.naver.com/research/market_info_list.naver",
            "db_name": "MarketConditions_report_chroma_db",
            "is_industry": False,
        },
        "company": {
            "name": "ì¢…ëª©",
            "url": "https://finance.naver.com/research/company_list.naver",
            "db_name": "Company_report_chroma_db",
            "is_industry": False,
        },
        "economy": {
            "name": "ê²½ì œ",
            "url": "https://finance.naver.com/research/economy_list.naver",
            "db_name": "Economy_report_chroma_db",
            "is_industry": False,
        },
        "industry": {
            "name": "ì‚°ì—…",
            "url": "https://finance.naver.com/research/industry_list.naver",
            "db_name": "Industry_report_chroma_db",
            "is_industry": True,  # ì‚°ì—… ë¦¬í¬íŠ¸ëŠ” ë ˆì´ì•„ì›ƒì´ ë‹¤ë¦„
        },
    }
    
    def __init__(
        self,
        vllm_base_url: str = "http://localhost:8001/v1",
        vllm_api_key: str = "vllm-key",
        vllm_model: str = "skt/A.X-4.0-Light",
        embedding_model: str = "dragonkue/snowflake-arctic-embed-l-v2.0-ko",
        chroma_base_dir: str = "./Chroma_db",
        max_text_length: int = 8000,
        summary_max_tokens: int = 1024,
        temperature: float = 0.3,
        debug: bool = True,
    ):
        """
        Args:
            vllm_base_url: vLLM ì„œë²„ URL
            vllm_api_key: API í‚¤
            vllm_model: ìš”ì•½ì— ì‚¬ìš©í•  ëª¨ë¸
            embedding_model: ì„ë² ë”© ëª¨ë¸
            chroma_base_dir: ChromaDB ì €ì¥ ë£¨íŠ¸ ê²½ë¡œ
            max_text_length: PDF í…ìŠ¤íŠ¸ ìë¥´ê¸° ê¸¸ì´ (ê¸°ë³¸ 8000ì)
            summary_max_tokens: ìš”ì•½ ìµœëŒ€ í† í° ìˆ˜
            temperature: LLM temperature
            debug: ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.vllm_base_url = vllm_base_url
        self.vllm_model = vllm_model
        self.embedding_model = embedding_model
        self.chroma_base_dir = chroma_base_dir
        self.max_text_length = max_text_length
        self.summary_max_tokens = summary_max_tokens
        self.temperature = temperature
        self.debug = debug
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = OpenAI(
            base_url=vllm_base_url,
            api_key=vllm_api_key
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (lazy loading)
        self._embeddings = None
        
        # HTTP í—¤ë”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        if self.debug:
            print(f"[NaverReportPipeline] ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"  - vLLM URL: {self.vllm_base_url}")
            print(f"  - ìš”ì•½ ëª¨ë¸: {self.vllm_model}")
            print(f"  - ì„ë² ë”©: {self.embedding_model}")
            print(f"  - í…ìŠ¤íŠ¸ ìë¥´ê¸°: {self.max_text_length}ì")
            print(f"  - ChromaDB: {self.chroma_base_dir}")
    
    def get_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì‹±ê¸€í†¤)"""
        if self._embeddings is None:
            if self.debug:
                print(f"[INFO] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embedding_model}")
            self._embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model
            )
        return self._embeddings
    
    def get_report_list(self, category: str) -> List[Dict]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì˜¤ëŠ˜ ë‚ ì§œ ë¦¬í¬íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            category: "market", "company", "economy", "industry"
        """
        if category not in self.CATEGORIES:
            print(f"[ERROR] ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬: {category}")
            return []
        
        cat_info = self.CATEGORIES[category]
        url = cat_info["url"]
        is_industry = cat_info["is_industry"]
        
        today_str = datetime.now().strftime("%y.%m.%d")
        
        if self.debug:
            print(f"\n{'='*60}")
            print(f"ğŸ“… [{cat_info['name']}] {today_str} ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
            print(f"{'='*60}")
        
        try:
            response = requests.get(url, headers=self.headers)
            response.encoding = 'euc-kr'
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = soup.select('table.type_1 tr')
            reports = []
            
            for row in rows:
                tds = row.select('td')
                if len(tds) < 4:
                    continue
                
                # ë‚ ì§œ ì°¾ê¸° (YY.MM.DD í˜•ì‹)
                report_date = ""
                for td in tds:
                    text = td.get_text(strip=True)
                    if len(text) == 8 and text.count('.') == 2:
                        report_date = text
                        break
                
                if report_date != today_str:
                    continue
                
                # ì œëª© ë° PDF ë§í¬ ì¶”ì¶œ
                title_idx = 1 if is_industry else 0
                title_tag = tds[title_idx].select_one('a')
                pdf_link_tag = row.select_one('a[href*=".pdf"]')
                
                if pdf_link_tag and title_tag:
                    reports.append({
                        'title': title_tag.get_text(strip=True),
                        'pdf_url': pdf_link_tag['href'],
                        'date': report_date,
                        'category': category
                    })
            
            if self.debug:
                print(f"âœ… {len(reports)}ê°œ ë¦¬í¬íŠ¸ ë°œê²¬")
            
            return reports
            
        except Exception as e:
            print(f"[ERROR] {cat_info['name']} ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_text_from_pdf(self, pdf_url: str) -> Optional[str]:
        """PDF URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            response = requests.get(pdf_url, headers=self.headers, timeout=30)
            with fitz.open(stream=response.content, filetype="pdf") as doc:
                text = "".join([page.get_text() for page in doc])
                return text[:self.max_text_length]  # ì„¤ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
        except Exception as e:
            if self.debug:
                print(f"[ERROR] PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def summarize_text(self, text: str, title: str = "", date: str = "") -> str:
        """
        í…ìŠ¤íŠ¸ ìš”ì•½ (ë‚ ì§œì™€ ì œëª© í¬í•¨)
        
        Args:
            text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            title: ë¦¬í¬íŠ¸ ì œëª©
            date: ë¦¬í¬íŠ¸ ë‚ ì§œ (YY.MM.DD)
        
        Returns:
            "[ë‚ ì§œ] ì œëª©\nìš”ì•½ë‚´ìš©" í˜•ì‹
        """
        if not text or len(text) < 100:
            return "ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì ì–´ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            response = self.client.chat.completions.create(
                model=self.vllm_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¦¬í¬íŠ¸ ë‚´ìš©ì„ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": f"ë¦¬í¬íŠ¸ ë‚´ìš©:\n{text}"
                    }
                ],
                temperature=self.temperature,
                max_tokens=self.summary_max_tokens
            )
            
            summary_content = response.choices[0].message.content
            
            # ë‚ ì§œì™€ ì œëª©ì„ ìš”ì•½ ì•ì— ì¶”ê°€
            header = ""
            if date:
                # YY.MM.DD -> 20YY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë” ì½ê¸° ì‰½ê²Œ)
                try:
                    year, month, day = date.split('.')
                    full_date = f"20{year}-{month}-{day}"
                    header += f"[{full_date}]"
                except:
                    header += f"[{date}]"
            
            if title:
                header += f" {title}"
            
            if header:
                return f"{header}\n\n{summary_content}"
            else:
                return summary_content
            
        except Exception as e:
            return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}"
    
    def save_to_chromadb(
        self,
        summaries: List[Dict],
        category: str
    ) -> Optional[Chroma]:
        """ìš”ì•½ ê²°ê³¼ë¥¼ ChromaDBì— ì €ì¥"""
        if not summaries:
            print(f"[WARN] {category}: ì €ì¥í•  ìš”ì•½ë³¸ ì—†ìŒ")
            return None
        
        cat_info = self.CATEGORIES[category]
        db_path = os.path.join(self.chroma_base_dir, cat_info["db_name"])
        
        # Document ìƒì„±
        docs = []
        for item in summaries:
            content = item.get("summary", "")
            if not content or "ì˜¤ë¥˜" in content:
                continue
            
            metadata = {
                "title": item.get("title", "ì œëª© ì—†ìŒ"),
                "date": item.get("date", ""),
                "source": item.get("pdf_url", ""),
                "category": category
            }
            docs.append(Document(page_content=content, metadata=metadata))
        
        if not docs:
            print(f"[WARN] {category}: ìœ íš¨í•œ ìš”ì•½ë³¸ ì—†ìŒ")
            return None
        
        if self.debug:
            print(f"\nğŸ’¾ [{cat_info['name']}] ChromaDB ì €ì¥ ì¤‘...")
            print(f"   ê²½ë¡œ: {db_path}")
            print(f"   ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        try:
            embeddings = self.get_embeddings()
            vectorstore = Chroma.from_documents(
                documents=docs,
                embedding=embeddings,
                persist_directory=db_path
            )
            
            if self.debug:
                print(f"âœ… [{cat_info['name']}] ì €ì¥ ì™„ë£Œ")
            
            return vectorstore
            
        except Exception as e:
            print(f"[ERROR] {cat_info['name']} DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def process_category(
        self,
        category: str,
        save_json: bool = True
    ) -> Optional[Chroma]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            category: "market", "company", "economy", "industry"
            save_json: JSON íŒŒì¼ë¡œë„ ì €ì¥í• ì§€ ì—¬ë¶€
        
        Returns:
            ChromaDB vectorstore ë˜ëŠ” None
        """
        cat_info = self.CATEGORIES[category]
        
        # 1. ë¦¬í¬íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        report_list = self.get_report_list(category)
        
        if not report_list:
            print(f"[INFO] {cat_info['name']}: ì˜¤ëŠ˜ ë‚ ì§œ ë¦¬í¬íŠ¸ ì—†ìŒ")
            return None
        
        # 2. PDF ì¶”ì¶œ ë° ìš”ì•½
        summaries = []
        for i, report in enumerate(report_list, 1):
            if self.debug:
                print(f"\n[{i}/{len(report_list)}] {report['title'][:50]}...")
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = self.extract_text_from_pdf(report['pdf_url'])
            
            if not full_text:
                continue
            
            # ìš”ì•½ (ì œëª©ê³¼ ë‚ ì§œ í¬í•¨)
            summary = self.summarize_text(
                full_text,
                title=report['title'],
                date=report['date']
            )
            
            summaries.append({
                "title": report['title'],
                "pdf_url": report['pdf_url'],
                "date": report['date'],
                "summary": summary,
                "category": category
            })
            
            if self.debug:
                print(f"   âœ… ìš”ì•½ ì™„ë£Œ: {summary[:80]}...")
            
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        # 3. JSON ì €ì¥ (ì„ íƒ)
        if save_json and summaries:
            json_filename = f"{category}_summaries_{datetime.now().strftime('%Y%m%d')}.json"
            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(summaries, f, ensure_ascii=False, indent=2)
            
            if self.debug:
                print(f"\nğŸ“„ JSON ì €ì¥: {json_filename}")
        
        # 4. ChromaDB ì €ì¥
        vectorstore = self.save_to_chromadb(summaries, category)
        
        return vectorstore
    
    def process_all_categories(self, save_json: bool = True):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¼ê´„ ì²˜ë¦¬"""
        print(f"\n{'='*70}")
        print(f"ğŸš€ ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬í¬íŠ¸ í†µí•© ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'='*70}")
        
        results = {}
        
        for category in self.CATEGORIES.keys():
            try:
                vectorstore = self.process_category(category, save_json)
                results[category] = vectorstore
            except Exception as e:
                print(f"\n[ERROR] {category} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                results[category] = None
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*70}")
        
        for category, vs in results.items():
            cat_name = self.CATEGORIES[category]['name']
            status = "âœ… ì™„ë£Œ" if vs else "âŒ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ"
            print(f"  {cat_name:8s}: {status}")
        
        print(f"\nâœ¨ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n")
        
        return results
    
    def test_search(self, category: str, query: str, k: int = 3):
        """ì €ì¥ëœ DB ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        cat_info = self.CATEGORIES[category]
        db_path = os.path.join(self.chroma_base_dir, cat_info["db_name"])
        
        if not os.path.exists(db_path):
            print(f"[ERROR] DB ì—†ìŒ: {db_path}")
            return
        
        print(f"\nğŸ” [{cat_info['name']}] ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        print(f"   ì§ˆë¬¸: {query}")
        print(f"   DB: {db_path}")
        
        try:
            embeddings = self.get_embeddings()
            vectorstore = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )
            
            results = vectorstore.similarity_search(query, k=k)
            
            print(f"\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
            for i, doc in enumerate(results, 1):
                print(f"\n[{i}] ì œëª©: {doc.metadata['title']}")
                print(f"    ë‚ ì§œ: {doc.metadata.get('date', 'N/A')}")
                print(f"    ë‚´ìš©: {doc.page_content[:150]}...")
        
        except Exception as e:
            print(f"[ERROR] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")


# ============================================================
# ì‹¤í–‰ ì˜ˆì‹œ
# ============================================================

if __name__ == "__main__":
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = NaverReportPipeline(
        vllm_base_url="http://localhost:8001/v1",
        vllm_model="skt/A.X-4.0-Light",
        vllm_api_key = "vllm-key",  # ìˆ˜ì • 
        embedding_model="dragonkue/snowflake-arctic-embed-l-v2.0-ko", # ìˆ˜ì •
        chroma_base_dir="./Chroma_db",
        max_text_length=8000,      # PDF 8000ìê¹Œì§€ë§Œ ì½ê¸°
        summary_max_tokens=1024,   # ìš”ì•½ ìµœëŒ€ í† í°
        temperature=0.3,
        debug=True,
    )
    
    # ë°©ë²• 1: ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¼ê´„ ì²˜ë¦¬
    results = pipeline.process_all_categories(save_json=True)
    
    # ë°©ë²• 2: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬
    # pipeline.process_category("market")
    # pipeline.process_category("company")
    # pipeline.process_category("economy")
    # pipeline.process_category("industry")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # pipeline.test_search("market", "ì—”í™”ì— ëŒ€í•œ ë¶„ì„", k=2)
    # pipeline.test_search("company", "ì‚¼ì„±ì „ì", k=2)
    # pipeline.test_search("economy", "ì¤‘êµ­ GDP", k=2)
    # pipeline.test_search("industry", "ì›ì „ ì‚°ì—…", k=2)