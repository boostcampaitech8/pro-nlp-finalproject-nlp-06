from __future__ import annotations

import re
import time
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, urljoin

import requests
from bs4 import BeautifulSoup
import pendulum
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

import re
from typing import List, Tuple


BASE_URL = "https://finance.naver.com/news/mainnews.naver"
KST = pendulum.timezone("Asia/Seoul")


def now_kst() -> datetime:
    return datetime.now(tz=KST)


def _attach_kst(dt_naive: datetime) -> datetime:
    return dt_naive.replace(tzinfo=KST)


def to_nnews_link(link: str) -> str:
    link_full = urljoin("https://finance.naver.com", link)
    parsed = urlparse(link_full)

    if parsed.netloc == "finance.naver.com" and parsed.path == "/news/news_read.naver":
        qs = parse_qs(parsed.query)
        article_id = qs.get("article_id", [""])[0]
        office_id = qs.get("office_id", [""])[0]
        if article_id and office_id:
            return f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"

    return link_full


def parse_datetime_full_kst(text: str):
    if not text:
        return None
    text = text.strip()
    try:
        dt = datetime.strptime(text, "%Y-%m-%d %H:%M:%S")
        return _attach_kst(dt)
    except Exception:
        return None


def parse_naver_datetime_fallback(text: str, base_now: datetime):
    if not text:
        return None

    text = text.strip()
    year = base_now.year
    month = base_now.month
    day = base_now.day

    try:
        if re.match(r"^\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2}$", text):
            dt = datetime.strptime(text, "%Y.%m.%d %H:%M")
            return _attach_kst(dt)

        if re.match(r"^\d{2}\.\d{2}\s+\d{2}:\d{2}$", text):
            dt = datetime.strptime(f"{year}.{text}", "%Y.%m.%d %H:%M")
            return _attach_kst(dt)

        if re.match(r"^\d{2}:\d{2}$", text):
            dt = datetime.strptime(
                f"{year}.{month:02d}.{day:02d} {text}",
                "%Y.%m.%d %H:%M",
            )
            return _attach_kst(dt)
    except Exception:
        return None

    return None


def parse_article_time_kst(wdate: str, base_now: datetime):
    dt = parse_datetime_full_kst(wdate)
    if dt:
        return dt
    return parse_naver_datetime_fallback(wdate, base_now=base_now)


def crawl_article_content(url: str, session: requests.Session) -> str:
    url = to_nnews_link(url)

    try:
        res = session.get(url, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âš ï¸  ë³¸ë¬¸ ìš”ì²­ ì—ëŸ¬: {e} â†’ {url}")
        return ""

    soup = BeautifulSoup(res.text, "html.parser")

    tag = soup.select_one("article#dic_area")
    if tag:
        return tag.get_text("\n", strip=True)

    tag = soup.select_one("#newsct_article")
    if tag:
        return tag.get_text("\n", strip=True)

    tag = soup.select_one("div#dic_area") or soup.select_one("div.article_viewer")
    if tag:
        return tag.get_text("\n", strip=True)

    print(f"âš ï¸  ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨: {url}")
    return ""


def crawl_page(page: int, session: requests.Session, time_limit: datetime, base_now: datetime):
    res = session.get(BASE_URL, params={"page": page}, timeout=10)
    res.raise_for_status()

    soup = BeautifulSoup(res.text, "html.parser")
    articles = []
    blocks = soup.select("li.block1")

    for block in blocks:
        subject_tag = block.select_one("dd.articleSubject > a")
        if not subject_tag:
            continue

        title = subject_tag.get_text(strip=True)
        link = to_nnews_link(subject_tag.get("href", ""))

        summary = block.select_one("dd.articleSummary")
        press = ""
        wdate = ""

        if summary:
            press_tag = summary.select_one("span.press")
            date_tag = summary.select_one("span.wdate")
            press = press_tag.get_text(strip=True) if press_tag else ""
            wdate = date_tag.get_text(strip=True) if date_tag else ""

        article_time = parse_article_time_kst(wdate, base_now=base_now)

        if article_time and article_time < time_limit:
            return articles, True

        content = crawl_article_content(link, session)
        time.sleep(0.4)

        articles.append({
            "title": title,
            "link": link,
            "press": press,
            "date": wdate,
            "content": content,
        })

    return articles, False


def crawl_last_hours_raw(hours: int = 1, max_page: int = 50):  # âœ… max_page ì¦ê°€
    """
    í˜„ì¬ ì‹œê°(KST) ê¸°ì¤€ ìµœê·¼ `hours`ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘.
    """
    if not isinstance(hours, int) or hours <= 0:
        raise ValueError("hoursëŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    now = now_kst()
    time_limit = now - timedelta(hours=hours)

    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {time_limit.strftime('%Y-%m-%d %H:%M')} ~ {now.strftime('%Y-%m-%d %H:%M')} (ìµœê·¼ {hours}ì‹œê°„)")
    
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}

    all_data = []
    with requests.Session() as session:
        session.headers.update(headers)

        for page in range(1, max_page + 1):
            print(f"ğŸ” í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...", end=" ")
            articles, stop = crawl_page(page, session, time_limit=time_limit, base_now=now)
            all_data.extend(articles)
            print(f"âœ… {len(articles)}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(all_data)}ê°œ)")
            
            if stop:
                print(f"â¹ï¸  ì‹œê°„ ì œí•œ ë„ë‹¬. í¬ë¡¤ë§ ì¢…ë£Œ (ì´ {page}í˜ì´ì§€)")
                break
            time.sleep(0.8)

    # âœ… í†µê³„ ì¶œë ¥
    valid_content = [a for a in all_data if a.get("content", "").strip()]
    empty_content = len(all_data) - len(valid_content)
    
    print(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ:")
    print(f"   - ì´ ë‰´ìŠ¤: {len(all_data)}ê°œ")
    print(f"   - ë³¸ë¬¸ ìˆìŒ: {len(valid_content)}ê°œ")
    print(f"   - ë³¸ë¬¸ ì—†ìŒ: {empty_content}ê°œ")
    
    return all_data


def normalize_text(text: str) -> str:
    text = text.replace("\r", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]{2,}", " ", text)
    return text.strip()


def chunk_by_chars(
    text: str,
    chunk_size: int = 800,
    overlap: int = 120,
) -> List[Tuple[str, int, int]]:
    """
    ë¬¸ì ê¸°ë°˜ chunking (Korean í¬í•¨ ì•ˆì „).
    ë°˜í™˜: [(chunk_text, start_idx, end_idx), ...]
    """
    if chunk_size <= 0:
        raise ValueError("chunk_size must be > 0")
    if overlap < 0:
        raise ValueError("overlap must be >= 0")
    if overlap >= chunk_size:
        raise ValueError("overlap must be < chunk_size")

    text = normalize_text(text)
    if not text:
        return []

    chunks: List[Tuple[str, int, int]] = []
    n = len(text)
    start = 0

    while start < n:
        end = min(start + chunk_size, n)

        if end < n:
            window = text[start:end]
            candidates = [window.rfind(p) for p in [". ", "ã€‚", "ë‹¤.", "ë‹¤ ", "\n", " "]]
            cut = max(candidates)
            if cut >= int(chunk_size * 0.6):
                end = start + cut + 1

        chunk = text[start:end].strip()
        if chunk:
            chunks.append((chunk, start, end))

        if end == n:
            break

        start = max(0, end - overlap)

    return chunks


def save_news_to_vectorstore(news_list: list, db_path: str = "./Chroma_db/News_chroma_db"):
    """
    ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì²­í‚¹í•˜ì—¬ Chroma DBì— ì €ì¥í•©ë‹ˆë‹¤.
    ê° ì²­í¬ ì•ì— ë‚ ì§œì™€ ì œëª©ì„ ë¶™ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°•í™”í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ’¾ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹œì‘...")
    
    # 1. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    documents = []
    news_with_content = 0
    news_without_content = 0
    total_chunks_per_news = []

    for idx, item in enumerate(news_list, 1):
        content = item.get("content", "")
        if not content or not content.strip():
            news_without_content += 1
            continue

        news_with_content += 1
        
        # âœ… ë‚ ì§œì™€ ì œëª© ì •ë³´ ì¶”ì¶œ
        date_str = item.get("date", "").strip()
        title = item.get("title", "").strip()
        
        # 2. ì²­í‚¹ ìˆ˜í–‰
        chunks = chunk_by_chars(content, chunk_size=800, overlap=120)
        total_chunks_per_news.append(len(chunks))

        if idx <= 3:  # ì²˜ìŒ 3ê°œë§Œ ìƒì„¸ ë¡œê·¸
            print(f"   [{idx}] [{date_str}] '{title[:40]}...' â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        for chunk_text, start_idx, end_idx in chunks:
            # âœ… ì²­í¬ ì•ì— ë‚ ì§œì™€ ì œëª© ë¶™ì´ê¸°
            header_parts = []
            if date_str:
                header_parts.append(f"[{date_str}]")
            if title:
                header_parts.append(f"[ì œëª©: {title}]")
            
            if header_parts:
                chunk_with_header = " ".join(header_parts) + "\n" + chunk_text
            else:
                chunk_with_header = chunk_text
            
            meta = {
                "title": title,
                "link": item.get("link"),
                "press": item.get("press"),
                "date": date_str,
                "start_idx": start_idx,
                "end_idx": end_idx
            }
            documents.append(Document(page_content=chunk_with_header, metadata=meta))

    if not documents:
        print("âŒ ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # 3. Chroma DB ì €ì¥
    print(f"\nğŸ”„ ChromaDBì— ì„ë² ë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=db_path
    )
    
    # 4. í†µê³„ ì¶œë ¥
    print(f"\nâœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ğŸ“° ë‰´ìŠ¤ í†µê³„:")
    print(f"   - ë³¸ë¬¸ ìˆëŠ” ë‰´ìŠ¤: {news_with_content}ê°œ")
    print(f"   - ë³¸ë¬¸ ì—†ëŠ” ë‰´ìŠ¤: {news_without_content}ê°œ")
    print(f"\nğŸ“¦ ì²­í¬ í†µê³„:")
    print(f"   - ì´ ì²­í¬ ìˆ˜: {len(documents)}ê°œ")
    if total_chunks_per_news:
        avg_chunks = sum(total_chunks_per_news) / len(total_chunks_per_news)
        print(f"   - í‰ê·  ì²­í¬/ë‰´ìŠ¤: {avg_chunks:.1f}ê°œ")
        print(f"   - ìµœì†Œ ì²­í¬: {min(total_chunks_per_news)}ê°œ")
        print(f"   - ìµœëŒ€ ì²­í¬: {max(total_chunks_per_news)}ê°œ")
    print(f"\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(db_path)}")
    print(f"{'='*60}\n")
    
    return vectorstore



# --- ì‹¤í–‰ ì˜ˆì‹œ ---
if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ & ë²¡í„°DB ì €ì¥")
    print("="*60)
    
    # 1. ìµœê·¼ 72ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
    raw_news = crawl_last_hours_raw(hours=72, max_page=50)
    
    # 2. ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
    db = save_news_to_vectorstore(raw_news)
    
    print("\nâœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")